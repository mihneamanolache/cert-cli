{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, collect_set, size, desc, when, expr, regexp_replace, udf\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"JSON Analysis\").getOrCreate()\n",
    "\n",
    "workspace_dir = os.getcwd() \n",
    "json_path = os.path.join(workspace_dir, \"data\", \"*.json\")\n",
    "\n",
    "# Read the JSON files\n",
    "df = spark.read.option(\"multiline\", \"true\").json(json_path).cache()\n",
    "\n",
    "# Count rows in the DataFrame\n",
    "df_count = df.count()\n",
    "print(f\"Total Records: {df_count}\")\n",
    "\n",
    "# Extract certificate details\n",
    "out = (\n",
    "    df.select(explode(col(\"certificates\")).alias(\"certificate\"))\n",
    "    .select(\n",
    "        col(\"certificate.commonName\").alias(\"domain\"),\n",
    "        col(\"certificate.san\"),\n",
    "        col(\"certificate.address\"),\n",
    "        col(\"certificate.organization\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"domain\"),\n",
    "        explode(col(\"san\")).alias(\"alternate_domain\"),\n",
    "        col(\"address\"),\n",
    "        col(\"organization\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate \n",
    "classified = (\n",
    "    out.withColumn(\"domain\", regexp_replace(col(\"domain\"), r\"^\\*\\.\", \"\"))\n",
    "    .withColumn(\"is_self\", col(\"domain\") == col(\"alternate_domain\"))\n",
    "    .withColumn(\"is_subdomain\", expr(\"alternate_domain LIKE concat('%.', domain)\") & ~col(\"is_self\"))\n",
    "    .groupBy(\"domain\")\n",
    "    .agg(\n",
    "        collect_set(when(col(\"is_subdomain\") & ~col(\"is_self\"), col(\"alternate_domain\"))).alias(\"subdomains\"),\n",
    "        collect_set(when(~col(\"is_subdomain\") & ~col(\"is_self\"), col(\"alternate_domain\"))).alias(\"alternate_domains\"),\n",
    "        collect_set(\"organization\").alias(\"organizations\"),\n",
    "        collect_set(\"address\").alias(\"addresses\")\n",
    "    )\n",
    "    .withColumn(\"organizations_count\", size(col(\"organizations\")))\n",
    "    .withColumn(\"alternate_domains_count\", size(col(\"alternate_domains\")))\n",
    "    .orderBy(desc(\"alternate_domains_count\"))\n",
    ")\n",
    "\n",
    "classified.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "# UDF to normalize organization names\n",
    "def normalize_org_name(org):\n",
    "    if org is None:\n",
    "        return None\n",
    "    org = org.strip()                                           # Remove leading/trailing spaces\n",
    "    org = re.sub(r\",\", \"\", org)                                 # Remove commas\n",
    "    org = re.sub(r\"\\s+\", \" \", org)                              # Normalize multiple spaces\n",
    "    org = re.sub(r\"\\bLLC\\b\", \"LLC.\", org, flags=re.IGNORECASE)  # Standardize LLC\n",
    "    org = re.sub(r\"\\bINC\\b\", \"Inc.\", org, flags=re.IGNORECASE)  # Standardize Inc\n",
    "    org = re.sub(r\"\\bLTD\\b\", \"Ltd.\", org, flags=re.IGNORECASE)  # Standardize Ltd\n",
    "    org = re.sub(r\"\\bSKG\\b\", \"SKG\", org, flags=re.IGNORECASE)   # Keep SKG unchanged\n",
    "    org = \" \".join(word.capitalize() for word in org.split())   # Convert to title case\n",
    "    return org\n",
    "normalize_org_udf = udf(normalize_org_name, StringType())\n",
    "\n",
    "organizations = (\n",
    "    out.filter(col(\"organization\").isNotNull()) \n",
    "    .withColumn(\"organization\", normalize_org_udf(col(\"organization\")))\n",
    "    .withColumn(\"domain\", regexp_replace(col(\"domain\"), r\"^\\*\\.\", \"\"))\n",
    "    .withColumn(\"alternate_domain\", regexp_replace(col(\"alternate_domain\"), r\"^\\*\\.\", \"\"))\n",
    "    .groupBy(\"organization\")\n",
    "    .agg(\n",
    "        collect_set(\"domain\").alias(\"domains\"),\n",
    "        collect_set(\"alternate_domain\").alias(\"alternate_domains\"),\n",
    "        collect_set(\"address\").alias(\"addresses\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "organizations.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
